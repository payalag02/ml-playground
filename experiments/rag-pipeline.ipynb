{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/payalagarwal/miniconda3/envs/agentic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAG Pipeline (Production-Ready Version)\n",
    "\n",
    "Improvements:\n",
    "- Persistent ChromaDB storage\n",
    "- Clean OpenAI integration\n",
    "- Safer architecture\n",
    "- Better repo-ready structure\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready RAG pipeline using:\n",
    "    - sentence-transformers\n",
    "    - ChromaDB\n",
    "    - OpenAI API (optional generation)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        collection_name: str = \"documents\",\n",
    "        persist_directory: str = \"./chroma_store\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50,\n",
    "    ):\n",
    "        print(\"üöÄ Initializing Advanced RAG Pipeline...\")\n",
    "\n",
    "        # Embedding model\n",
    "        print(f\"üì¶ Loading embedding model: {embedding_model_name}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "        # Persistent ChromaDB client (IMPORTANT for real projects)\n",
    "        self.chroma_client = chromadb.Client(\n",
    "            Settings(\n",
    "                persist_directory=persist_directory,\n",
    "                anonymized_telemetry=False,\n",
    "                allow_reset=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"},\n",
    "        )\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "        print(\"‚úÖ Pipeline initialized!\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # TEXT CHUNKING\n",
    "    # ------------------------------------------------------------------\n",
    "    def chunk_text(self, text: str, metadata: Optional[Dict] = None) -> List[Dict]:\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "\n",
    "        step = self.chunk_size - self.chunk_overlap\n",
    "        if step <= 0:\n",
    "            raise ValueError(\"chunk_overlap must be smaller than chunk_size\")\n",
    "\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk_words = words[i : i + self.chunk_size]\n",
    "            if not chunk_words:\n",
    "                continue\n",
    "\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"text\": \" \".join(chunk_words),\n",
    "                    \"metadata\": metadata or {},\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if i + self.chunk_size >= len(words):\n",
    "                break\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # ADD DOCUMENTS\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_documents(self, documents: List[Dict[str, str]]):\n",
    "\n",
    "        print(f\"\\nüì• Processing {len(documents)} documents...\")\n",
    "\n",
    "        all_texts, all_metadatas, all_ids = [], [], []\n",
    "        total_chunks = 0\n",
    "\n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            chunks = self.chunk_text(doc[\"text\"], doc.get(\"metadata\", {}))\n",
    "\n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                all_texts.append(chunk[\"text\"])\n",
    "                all_metadatas.append(chunk[\"metadata\"])\n",
    "                all_ids.append(f\"doc_{doc_idx}_chunk_{chunk_idx}\")\n",
    "                total_chunks += 1\n",
    "\n",
    "        print(f\"üìÑ Created {total_chunks} chunks\")\n",
    "\n",
    "        print(\"üî¢ Generating embeddings...\")\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            all_texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,  # IMPORTANT for cosine similarity\n",
    "        )\n",
    "\n",
    "        print(\"üíæ Storing in vector database...\")\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=all_texts,\n",
    "            metadatas=all_metadatas,\n",
    "            ids=all_ids,\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Documents added successfully!\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # RETRIEVAL\n",
    "    # ------------------------------------------------------------------\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 3,\n",
    "        filter_metadata: Optional[Dict] = None,\n",
    "    ) -> List[Dict]:\n",
    "\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            query, normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=k,\n",
    "            where=filter_metadata,\n",
    "        )\n",
    "\n",
    "        retrieved = []\n",
    "        for idx in range(len(results[\"ids\"][0])):\n",
    "            retrieved.append(\n",
    "                {\n",
    "                    \"id\": results[\"ids\"][0][idx],\n",
    "                    \"text\": results[\"documents\"][0][idx],\n",
    "                    \"score\": 1 - results[\"distances\"][0][idx],\n",
    "                    \"metadata\": results[\"metadatas\"][0][idx],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return retrieved\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # PROMPT BUILDER\n",
    "    # ------------------------------------------------------------------\n",
    "    def generate_prompt(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "\n",
    "        context_parts = []\n",
    "\n",
    "        for idx, doc in enumerate(retrieved_docs, 1):\n",
    "            context_parts.append(f\"Document {idx}:\")\n",
    "            context_parts.append(doc[\"text\"])\n",
    "            context_parts.append(\"\")\n",
    "\n",
    "        context = \"\\n\".join(context_parts)\n",
    "\n",
    "        prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # FULL RAG QUERY\n",
    "    # ------------------------------------------------------------------\n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        k: int = 3,\n",
    "        filter_metadata: Optional[Dict] = None,\n",
    "    ) -> Dict:\n",
    "\n",
    "        print(f\"\\nüîç Query: {question}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        retrieved_docs = self.retrieve(question, k, filter_metadata)\n",
    "\n",
    "        for idx, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"Document {idx} | Score: {doc['score']:.3f}\")\n",
    "            print(doc[\"text\"][:200], \"...\\n\")\n",
    "\n",
    "        prompt = self.generate_prompt(question, retrieved_docs)\n",
    "\n",
    "        return {\n",
    "            \"query\": question,\n",
    "            \"retrieved_documents\": retrieved_docs,\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # LLM GENERATION \n",
    "    # ------------------------------------------------------------------\n",
    "    def generate_answer(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        model: str = \"gpt-4.1-mini\",\n",
    "        api_key: Optional[str] = None,\n",
    "    ) -> str:\n",
    "\n",
    "        api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # RESET COLLECTION\n",
    "    # ------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.chroma_client.delete_collection(self.collection.name)\n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            name=self.collection.name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"},\n",
    "        )\n",
    "        print(\"üóëÔ∏è Collection reset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Advanced RAG Pipeline...\n",
      "üì¶ Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 2053.76it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline initialized!\n",
      "\n",
      "üì• Processing 4 documents...\n",
      "üìÑ Created 4 chunks\n",
      "üî¢ Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Storing in vector database...\n",
      "‚úÖ Documents added successfully!\n",
      "\n",
      "üîç Query: Explain transformers\n",
      "================================================================================\n",
      "Document 1 | Score: 0.547\n",
      "Transformers are a type of neural network architecture introduced in the paper 'Attention Is All You Need'. They use self-attention mechanisms to process sequential data in parallel, making them much  ...\n",
      "\n",
      "Document 2 | Score: 0.104\n",
      "Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns  ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ LLM Answer:\n",
      " Transformers are a type of neural network architecture designed for processing sequential data. Introduced in the paper \"Attention Is All You Need,\" they utilize self-attention mechanisms that allow them to handle data in parallel rather than sequentially, making them much faster than traditional recurrent neural networks. This architecture has significantly advanced the field of natural language processing (NLP) and serves as the foundation for influential models such as BERT, GPT, and T5. Transformers are particularly effective for tasks including translation, summarization, and text generation.\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    {\n",
    "        'text': \"\"\"\n",
    "        Python is a high-level, interpreted programming language known for its \n",
    "        simplicity and readability. Created by Guido van Rossum and first released \n",
    "        in 1991, Python emphasizes code readability with significant whitespace. \n",
    "        It supports multiple programming paradigms including procedural, object-oriented, \n",
    "        and functional programming. Python is widely used in web development, data \n",
    "        science, artificial intelligence, scientific computing, and automation.\n",
    "        \"\"\",\n",
    "        'metadata': {'category': 'programming', 'language': 'python'}\n",
    "    },\n",
    "    {\n",
    "        'text': \"\"\"\n",
    "        Machine learning is a method of data analysis that automates analytical model \n",
    "        building. It is a branch of artificial intelligence based on the idea that \n",
    "        systems can learn from data, identify patterns and make decisions with minimal \n",
    "        human intervention. Applications include recommendation systems, image recognition, \n",
    "        natural language processing, and predictive analytics. Popular ML frameworks \n",
    "        include TensorFlow, PyTorch, and scikit-learn.\n",
    "        \"\"\",\n",
    "        'metadata': {'category': 'ai', 'subtopic': 'machine-learning'}\n",
    "    },\n",
    "    {\n",
    "        'text': \"\"\"\n",
    "        Vector databases are specialized databases designed to store and query high-dimensional \n",
    "        vectors efficiently. They use approximate nearest neighbor (ANN) algorithms like \n",
    "        HNSW or IVF to enable fast similarity search. Vector databases are crucial for \n",
    "        modern AI applications including semantic search, recommendation engines, and \n",
    "        retrieval-augmented generation (RAG) systems. Popular options include Pinecone, \n",
    "        Weaviate, Milvus, and ChromaDB.\n",
    "        \"\"\",\n",
    "        'metadata': {'category': 'databases', 'subtopic': 'vector-db'}\n",
    "    },\n",
    "    {\n",
    "        'text': \"\"\"\n",
    "        Transformers are a type of neural network architecture introduced in the paper \n",
    "        'Attention Is All You Need'. They use self-attention mechanisms to process \n",
    "        sequential data in parallel, making them much faster than recurrent neural networks. \n",
    "        Transformers have revolutionized NLP and are the foundation of models like BERT, \n",
    "        GPT, and T5. They excel at tasks like translation, summarization, and text generation.\n",
    "        \"\"\",\n",
    "        'metadata': {'category': 'ai', 'subtopic': 'deep-learning'}\n",
    "    }\n",
    "]\n",
    "    \n",
    "# Initialize pipeline\n",
    "\n",
    "rag = RAGPipeline(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30,\n",
    ")\n",
    "\n",
    "rag.add_documents(documents)\n",
    "\n",
    "result = rag.query(\n",
    "    \"Explain transformers\",\n",
    "    k=2,\n",
    "    filter_metadata={\"category\": \"ai\"},\n",
    ")\n",
    "\n",
    "answer = rag.generate_answer(result[\"prompt\"])\n",
    "print(\"\\nü§ñ LLM Answer:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
